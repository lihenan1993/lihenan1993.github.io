---
title: 第01周：微服务（微服务概览与治理）
category: JK-Go
typora-root-url: ..
---

### 微服务概览

- 单体架构

  尽管也是模块化，但是最终会打包为单体式应用。

  问题是，应用太复杂，无法扩展，可靠性很低，敏捷性开发和部署变得无法完成。

  

- SOA（面向服务）的架构模式和微服务的关系。

  微服务是SOA的一种实践，微服务也是面向服务的架构，不过有自己的一些特殊定义。

  1. 小即是美

     小的服务代码少，bug少，易测试，易维护。也更容易不断迭代完善。

  2. 单一职责

     一个服务也只需要做好一件事，专注才能做好

  3. 尽可能早地创建原型：尽可能早的提供API，达成服务间沟通的一致性约定，再慢慢实现

  4. 可移植效率更高：兼容不同平台

#### 微服务的定义

围绕业务功能构建的，服务**关注单一业务**，服务间采用**轻量级的通信机制**，可以全自动独立部署，可以使用**不同的编程语言**和数据存储技术。

通过组件快速开发系统，使得整个系统变得清晰灵活。

- 原子服务 
- 独立进程
- 隔离部署
- 去中心化服务治理

缺点

- 基础设置的建设、复杂度高
- 联调困难

#### 微服务的不足

1. 微服务是分布式系统，不得不用RPC或者消息传递，来实现进程间通讯；

   例如： A服务 请求 B服务 10次 ， 而B服务需要请求C服务10次， 这样c服务的请求被100倍放大，导致网络压力大增。 解决办法， 批量请求和返回。

2. 必须要写代码来处理消息传递中速度过慢或者服务不可用等局部失效问题

   例如： ABC请求 ，耗时是A+B+C ，其实很多时候没必要串行调用，微服务可以并行调用。服务降级

3. 分区的数据库架构，同时更新多个业务主题的事务很普遍。保证数据一致性，挑战很大

4. 测试很复杂，不知道如何联调

5. 服务模块间有依赖，升级可能会波及多个服务模块的修改

6. 对运维基础设施的挑战比较大。报警系统，资源指标，部署难度

#### 组件服务化

1. Kit 一个微服务的基础库（框架）
2. service： 业务代码+ kit +第三方依赖组成的微服务
3. rpc + mq ： 轻量级通讯

#### 按业务组织服务

1. 自己开发，自己部署，自己修复。对业务负责。测试和运维提供，测试平台和数据收集平台，方便开发更快定位自己的问题
2. 比如订单服务，而不是能是数据访问服务

#### 去中心化

- 数据去中心化

  每个微服务独享自身的数据存储设施（缓存，数据库）

- 治理去中心化

  流量有热点，比如账号服务是几乎所有微服务的热点。

  服务发现

- 技术去中心化

  可以选用各种语言，但是推荐语言收敛

#### 基础设施自动化

无自动化不微服务，自动化包括测试和部署。

CICD： Gitlab +　Gitlab Hooks + k8s

Testing： 测试环境，单元测试（go test），API自动化测试（yapi）

在线运行时：k8s，Prometheus、ELK、Conrtol Panle

#### 可用性 和 兼容性设计

1. 所有依赖都会有问题，增强鲁棒性
2. 采用粗粒度的进程间通信，比如批量接口
3. 可用性
   - 隔离
   - 超时控制
   - 负载保护
   - 限流
   - 降级
   - 重试
   - 负载均衡
4. 时刻谨记保持服务接口的兼容性，发送时要保守（最小限度数据），接受时要开放（对数据最大宽容度校验和允许冗余）。 伯斯塔尔法则



### 微服务设计

#### API Gateway

API一定要**面向用户场景**，不要面向业务。



##### 1.0

根据功能垂直划分，对外暴露一批微服务，这样缺乏统一的出口面临很多问题

- 客户端直接到微服务通信，强耦合。
  - 如果客户端不升级，微服务没法大规模重构
- 客户端需要多次请求，不同微服务接口，来聚合数据。
  - 沟通成本极大
  - 不同微服务接口不统一
  - 延迟高
- 协议不统一
  - 需要端来适应
- 面向端的API适配，耦合到了内部服务
  - 比如IOS需要字段A，安卓需要字段B，这些都得耦合到微服务
- 统一逻辑无法收敛
  - 比如安全校验、限流每个微服务都得去做

![image-20201213172450882](/assets/img/image-20201213172450882.png)

##### 2.0

为了解决以上问题，抽象一个app-interface，用于统一的协议出口，内部进行数据聚合，按照业务场景来涉及粗粒度的API

优势：

- 轻量交互
- 协议服务：针对终端定制化API
- 动态升级：兼容原有系统的升级，更新服务而不是协议
- 沟通效率提升，移动业务 与 网关小组沟通

这种设计模式叫 BFF（Backend for Frontend 面向前端的后端），BFF是做数据的组装和编排的。

比如打开一个B站视频，有视频信息，有稿件信息，有Up主信息，有推荐信息，有评论等。

BFF就是调用这些微服务，然后把数据组装打包。返回给端



缺点：

- 如果app-interface出现问题，比如流量暴涨，代码缺陷，所有服务就崩溃了。

![image-20201213172502620](/assets/img/image-20201213172502620.png)

##### 3.0

继续拆，app-interface , app-view , app-*

这样单个模块业务会越来越复杂，交付低下

横切面逻辑，安全认证，日志监控，限流熔断，随着时间推移，代码越来越复杂

因为升级基础库，各个app-*都得升级，推动很难

![image-20201213172511017](/assets/img/image-20201213172511017.png)

##### 4.0

引入API-Gateway

横跨切面的功能，全部放入API Gateway

把业务集成度高的BFF层和通用功能服务层API Gateway进行分层处理

网关功能是解耦拆分和后续升级迁移，横跨切面功能拆分后，BFF更专注业务逻辑交付，实现了架构上的关注分离

![image-20201213172519541](/assets/img/image-20201213172519541.png)

#### 微服务划分

1. 根据公司部门提供的职能。例如客户服务部门提供客服服务的职能，财务部门提供财务相关的职能。
2. 领域驱动设计（Domain Driven Design，DDD）的限界上下文
3. DDD的CQRS：命令与查询职责分离模式

![image-20201213172532207](/assets/img/image-20201213172532207.png)

#### 微服务安全

对于外网请求，在API Gateway进行统一的认证拦截，一旦认证成功，用JWT通过RPC传递给BFF层，BFF校验Token完整性后把身份信息注入到BFF应用的Context中，BFF到其他下层的微服务，直接在RPC请求中带入用户UID

对于服务内部，一般要区分身份认证和授权

服务内部直接调用可以分为三种安全模式

1. 认证且通讯加密
2. 认证但通讯不加密
3. 无认证和通讯加密

![image-20201213172545007](/assets/img/image-20201213172545007.png)



### gRPC

gRPC (google Remote Procedure Call) **高性能开源统一RPC框架**

优点：

1. 多语言：语言中立，支持多种语言。 

2. 轻量级、高性能：序列化支持 PB(Protocol Buffer)和 JSON，PB 是一种语言无关的高性能序列化框架。

3. 移动端：**基于标准的 HTTP2 设计，支持双向流、消息头压缩、单 TCP 的多路复用**、服务端推送等特性， 这些特性使得 gRPC 在移动端设备上更加省电和节省 网络流量。

4. IDL：基于文件定义服务，通过 proto3 工具生成指定 语言的数据结构、服务端接口以及客户端 Stub。 

缺点：

1. 服务治理相关能力缺失

2. 负载均衡和服务发现等功能要业务扩展实现



#### HealthCheck 主动健康检测

频率为 15s 或 30s

Health checks用于探测服务器是否能够处理rpc请求。客户端到服务器的运行状况检查可以通过点对点或某些控制系统进行。服务器可能未准备好接受请求，正在关闭或其他原因，这时他会选择答复“unhealthy”。如果在某个时间段内未收到响应或响应说不健康，则客户端可以采取相应的措施。

GRPC服务可以用作简单的客户端到服务器方案和其他控制系统（例如负载平衡）的运行状况检查机制。grpc作为一个高级服务有这么些优点：第一，由于它本身是GRPC服务，因此进行健康检查的格式与普通rpc相同。第二，它具有丰富的语义，例如每个服务的健康状态。第三，作为GRPC服务，it is able reuse all the existing billing, quota infrastructure, 因此服务器可以完全控制运行状况检查服务的访问。

https://github.com/grpc/grpc/blob/master/doc/health-checking.md

```protobuf
syntax = "proto3";

package grpc.health.v1;

message HealthCheckRequest {
  string service = 1;
}

message HealthCheckResponse {
  enum ServingStatus {
    UNKNOWN = 0;
    SERVING = 1;
    NOT_SERVING = 2;
    SERVICE_UNKNOWN = 3;  // Used only by the Watch method.
  }
  ServingStatus status = 1;
}

service Health {
  rpc Check(HealthCheckRequest) returns (HealthCheckResponse);

  rpc Watch(HealthCheckRequest) returns (stream HealthCheckResponse);
}
```



可以检测出来 提供商和服务发现是正常，但是消费者和提供商之间网络异常的情况。



### 服务发现

分布式系统中，程序通过一个标识来获取服务列表，服务列表是能随着服务状态更新的。

#### 客户端发现

1. 客户端使用负载均衡算法，从服务注册表中，选择可用的服务实例

2. 一个服务实例被启动时，它的网络地址会被写到注册表上

3. 当服务实例下线时，再从注册表中删除

4. 注册表通过心跳机制动态刷新

   ![image-20201216173825561](/assets/img/image-20201216173825561.png)

#### 服务端发现

客户端直接向服务注册中心进行请求，服务注册中心再通过自身负载均衡策略，将请求路由到可用的服务实例上。



#### 对比

| 客户端模式                                                   | 服务端模式                                                   |
| :----------------------------------------------------------- | :----------------------------------------------------------- |
| 只需要周期性获取列表，在调用服务时可以直接调用少了一个网络跳转。但需要在每个客户端维护获取列表和负载均衡逻辑。 | 简单                                                         |
| 可用性高，即使注册中心出现故障也能正常工作                   | 可用性由路由器中间件决定，路由中间件故障则所有服务不可用，同时，由于所有调度以及存储都由中间件服务器完成，**中间件服务器可能会面临过高的负载** |
| 服务上下线对调用方有影响（会出现短暂调用失败）               | 服务上下线调用方无感知                                       |

> 微服务的核心是去中心化，我们使用客户端发现模式



#### 各种服务发现对比

| Feature              | Consul                 | zookeeper             | etcd              | euerka                                                       |
| :------------------- | :--------------------- | :-------------------- | :---------------- | :----------------------------------------------------------- |
| 服务健康检查         | 服务状态，内存，硬盘等 | (弱)长连接，keepalive | 连接心跳          | 可配支持                                                     |
| 多数据中心           | 支持                   | —                     | —                 | —                                                            |
| kv存储服务           | 支持                   | 支持                  | 支持              | —                                                            |
| 一致性算法           | raft                   | paxos                 | raft              | —                                                            |
| cap                  | cp                     | cp                    | cp                | ap                                                           |
| 使用接口(多语言能力) | 支持http和dns          | 客户端                | http/grpc         | http（[sidecar](https://www.baidu.com/s?wd=sidecar&tn=24004469_oem_dg&rsv_dl=gh_pl_sl_csd)） |
| watch支持            | 全量/支持long polling  | 支持                  | 支持 long polling | 支持 long polling/大部分增量                                 |
| 自身监控             | metrics                | —                     | metrics           | metrics                                                      |
| 安全                 | acl /https             | acl                   | https支持（弱）   | —                                                            |
| spring cloud集成     | 已支持                 | 已支持                | 已支持            | 已支持                                                       |

根据实际应用场景选择服务发现组件

如果是海量服务发现和注册，服务状态可以弱一致，需要的是AP系统

AP系统可能出现的问题，牺牲强一致性，达到最终一致性：

1. 注册的事件延迟
   1. 没事，用旧的
2. 注销的时间延迟
   1. 客户端有健康检查
   2. 负载均衡
   3. 重试



#### euerka服务发现过程

1. 定位资源的方式

   appid + port

   business.service.method : port

   还可以附加更多的元数据：权重、染色标签、集群等

2. 服务提供商注册后，euerka节点 30s心跳一次；

   注册，心跳，下线，euerka节点都需要进行同步，注册和下线需要进行长轮询推送

   注意点：

   新启动euerka节点，需要同步数据，JVM预热

   euerka节点故障时，提供商不建议重启和发布

3. Server 定期(60s) 检测失效(90s)的实例，失效 则剔除。短时间里丢失了大量的心跳连接(15 分钟内心跳低于期望值*85%)，开启自我保护， 保留过期服务不删除。



### 多集群

L0服务，多集群必要性

1. 单一集群 N+2 的方式来冗余姐弟那
2. 单一集群可能故障，需要多集群
3. 单个机房内的机房故障导致问题

多集群实现：

给某个appid服务建立多集群，元数据加入集群。

集群的设置通过k8s启动时候，读取环境变量

多集群的演变

1. 不同服务，直播，游戏用不同集群，集群切换时候，导致缓存命中率低，因为玩游戏的人可能很少看直播，用户没有正交。
   - 解决办法，微服务同时连接多个集群，让所有数据热起来。C连 C01 和 C02
   - ![image-20201216211753662](/assets/img/image-20201216211753662.png)
2. 微服务太多，长连接导致的内存和CPU开销，HealthCheck可以高达30%。但是短链接会导致极大的资源成本和延迟
   - 解决办法，选取合适的子集大小和选择算法
   - 通常20-100个后端，部分场景需要大子集， 比如大批量读写操作。 • 后端平均分给客户端。 • 客户端重启，保持重新均衡，同时对后端重启保 持透明，同时连接的变动最小。
   - ![image-20201216212122497](/assets/img/image-20201216212122497.png)



### 多租户

在一个微服务架构中允许**多系统共存**是利用微服务稳定性以及模块化最有效的方式之一。

这种方式被成为多租户，租户可以是测试，金丝雀发布（灰度测试），影子系统

核心就是：

在RPC消息中携带元数据，元数据标识出路由到特殊的染色节点

数据库、缓存、消息队列等做线上的影子系统。

外网请求抹除掉元数据





> 灰度发布又叫金丝雀发布，灰度是指在黑与白之间，能够平滑过渡的一种发布方式。简单来讲，一个产品，需要快速迭代开发上线，一方面要保证质量，另一方面保证刚上线的系统，这时候就需要设计一套灰度发布系统，让一部分用户继续使用产品A，另一部用户使用产品B，一旦出现问题可以很快的控制影响，如果用户对B没有什么反对意见，那么逐步扩大范围，把所有用户都迁移到B上面来。
>
> 灰度发布可以保证整体系统的稳定，在初始灰度的时候就可以发现、调整问题，以保证其影响度。
>
> 为什么叫金丝雀发布（Canary）？
>
> 以前，旷工开矿，在下矿洞前需要检查下方是否有毒气，矿工们先会放一只金丝雀进去探是否有有毒气体，看金丝雀能否活下来。